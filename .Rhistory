ggplot(data=df_vm,aes(x = año, y = numero_suicidios)) +
geom_point(color = "skyblue") + facet_wrap(~pais)
# Agrupamos por pais,año y genero
df_v <- aggregate(numero_suicidios ~ pais + año + genero, data = df, FUN = sum)
# Seleccionamos solo las mujeres
df_vf <- subset(df_v, genero == "female")
# Graficamos mujeres
ggplot(data=df_vf,aes(x = año, y = numero_suicidios)) +
geom_point(color = "pink") + facet_wrap(~pais)
# Agrupamos por edad y año
df_v <- aggregate(numero_suicidios ~ edad + año, data = df, FUN = sum)
# Graficamos totalidad
ggplot(data=df_v,aes(x = año, y = numero_suicidios)) +
geom_point(color = "firebrick") + facet_wrap(~edad)
# Agrupamos por edad, año y genero
df_v <- aggregate(numero_suicidios ~ edad + año + genero, data = df, FUN = sum)
# Seleccionamos el genero "male"
df_vm <- subset(df_v, genero == "male")
# Graficamos hombres
ggplot(data=df_vm,aes(x = año, y = numero_suicidios)) +
geom_point(color = "blue") + facet_wrap(~edad)
# Agrupamos por edad,año y genero
df_v <- aggregate(numero_suicidios ~ edad + año + genero, data = df, FUN = sum)
# Seleccionamos el genero "female"
df_vf <- subset(df_v, genero == "female")
# Graficamos mujeres
ggplot(data=df_vf,aes(x = año, y = numero_suicidios)) +
geom_point(color = "red") + facet_wrap(~edad)
# Agrupamos por edad y pais
df_v <- aggregate(numero_suicidios ~ edad + pais, data = df, FUN = sum)
# Graficamos totalidad
ggplot(data=df_v,aes(x = edad, y = numero_suicidios)) +
geom_point(color = "firebrick") + facet_wrap(~pais) +
theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# Quitamos paises extremos
df_s <- subset(df, !(pais %in% c("United States","Japan","Russian Federation")))
# Agrupamos por pais y año
df_v <- aggregate(numero_suicidios ~ año + pais, data = df_s, FUN = sum)
# Graficamos totalidad
ggplot(data=df_v,aes(x = año, y = numero_suicidios)) +
geom_point(color = "firebrick") + facet_wrap(~pais)
# Agrupamos por PIB y año
df_v <- aggregate(numero_suicidios ~ PIB + año, data = df, FUN = sum)
# Graficamos totalidad
ggplot(data=df_v,aes(x = PIB, y = numero_suicidios)) +
geom_point(color = "firebrick") + facet_wrap(~año) + geom_smooth(method = lm,color = "firebrick")
# Agrupamos por poblacion y año
df_v <- aggregate(numero_suicidios ~ poblacion + año, data = df, FUN = sum)
# Graficamos totalidad
ggplot(data=df_v,aes(x = poblacion, y = numero_suicidios)) +
geom_point(color = "firebrick") + facet_wrap(~año) + geom_smooth(method = lm,color = "firebrick")
# Agrupamos por PIB per capita y año
df_v <- aggregate(numero_suicidios ~ PIB_per_capita + año, data = df, FUN = sum)
# Graficamos totalidad
ggplot(data=df_v,aes(x = PIB_per_capita, y = numero_suicidios)) +
geom_point(color = "firebrick") + facet_wrap(~año) + geom_smooth(method = lm,color = "firebrick")
# Agrupamos por generacion y año
df_v <- aggregate(numero_suicidios ~ generacion + año, data = df, FUN = sum)
# Graficamos totalidad
ggplot(data=df_v,aes(x = año, y = numero_suicidios)) +
geom_point(color = "skyblue") + facet_wrap(~generacion)
# Agrupamos por generacion, año y genero
df_v <- aggregate(numero_suicidios ~ generacion + año + genero, data = df, FUN = sum)
# Seleccionamos hombres
df_vm <- subset(df_v, genero == "male")
# Graficamos hombres
ggplot(data=df_vm,aes(x = año, y = numero_suicidios)) +
geom_point(color = "blue") + facet_wrap(~generacion)
# Agrupamos por generacion, año y genero
df_v <- aggregate(numero_suicidios ~ generacion + año + genero, data = df, FUN = sum)
# Seleccionamos mujeres
df_vf <- subset(df_v, genero == "female")
# Graficamos mujeres
ggplot(data=df_vf,aes(x = año, y = numero_suicidios)) +
geom_point(color = "pink") + facet_wrap(~generacion)
# Sometemos a funcion factor para incorporar a male o female sus respectivos valores numericos
df$genero <- factor(df$genero, levels = c("male","female"))
df$genero <- as.numeric(df$genero)
# Mostramos las 5 primeras lineas
head(df, n = 5)
# Creamos unas lista que contendra clave: "intervalo de edad", valor: media del intervalo
intervalos <- list()
# Seleccionamos todos los valores posibles de intervalos
edades <- unique(df$edad)
# Iteramos sobre los intervalos y extraemos la media
for (valor in edades){
partes <- strsplit(valor, "-")[[1]]
edad_inicial <- as.integer(gsub("\\+ years","",partes[1]))
edad_final <- as.integer(gsub(" years","",partes[2]))
if(is.na(edad_final)){
media_edad <- edad_inicial
} else{
suma_edades <- sum(edad_inicial:edad_final)
total_edades <- edad_final - edad_inicial
media_edad <- suma_edades/total_edades
}
# Una vez conseguida la media asignamos las media a su intervalo
intervalos[[valor]] <- as.integer(media_edad)
}
# Sustituimos en la variable edad los intervalos por sus medias
df$edad <- sapply(df$edad, function(edad) intervalos[[edad]])
# Graficamos las primeras 5 lineas
head(df, n = 5)
# Creamos listado de valores unicos de generacion
generaciones <- unique(df$generacion)
# Lo transformamos a valores numericos
df$generacion <- factor(df$generacion, levels = generaciones)
df$generacion <- as.numeric(df$generacion)
# Mostramos el dataset
head(df)
# Seleccionamos todas las variables menos paises
df_v <- df[, -1]
# Elaboramos matriz de correlaciones
res <- cor(df_v)
# Graficamos matriz de correlaciones
corrplot(res,method="color",tl.col="black", tl.srt=30, order = "AOE",
number.cex=0.75,sig.level = 0.01, addCoef.col = "black")
# Seleccionamos lista de paises unicos en dataset
paises <- unique(df$pais)
# Transformamos mediante factor los valores categoricos a numericos
df$pais <- factor(df$pais, levels = paises)
df$pais <- as.numeric(df$pais)
# Mostramos las 5 primeras lineas del nuevo dataset con la columna cambiada
head(df, n = 5)
# Volvemos a crear matriz de correlaciones ahroa con el pais
res <- cor(df)
# Graficamos la matriz de correlaciones
corrplot(res,method="color",tl.col="black", tl.srt=30, order = "AOE",
number.cex=0.75,sig.level = 0.01, addCoef.col = "black")
# Normalizamos el dataframe
df_v <- scale(df)
# Calculamos las matrices simples de A mediante la funcion svd
svd <- svd(df_v)
# Extrameos U
svd$v
# Calculamos los componentes principales de la matriz y normalizamos esta
pca <- prcomp(df, scale = TRUE)
# Establecemos las diferencias entre las 2 matrices de componentes principales
dif <- svd$v[,1:10] - pca$rotation[,1:10]
# Vemos los valores obtenidos
summary(dif)
# Calculamos los autovalores
singular_values <- svd$d^2
# Calculamos el porcentaje de varianza explicada por cada componente
varianza_explicada <- singular_values / sum(singular_values) * 100
# Graficamos el porcentaje de varianza explicada acumulada
plot(cumsum(varianza_explicada), xlab = "Número de componentes",
ylab = "Porcentaje de varianza explicada",
type = "b", pch = 19, col = "blue")
# Numero de componentes para explicar un 90% de la varianza
num_componentes <- which(cumsum(varianza_explicada) >= 90)[1]
cat("El numero de componentes principales para representar el 90% es de", num_componentes)
# Numero de componentes para explicar un 90% de la varianza
num_componentes <- which(cumsum(varianza_explicada) >= 80)[1]
cat("El numero de componentes principales para representar el 80% es de", num_componentes)
# Importamos las librerias en caso de necesitarlas
if(!require("stats")){
install.packages("stats")
library(stats)
}else{
library(stats)
}
if(!require("cluster")){
install.packages("cluster")
library(cluster)
}else{
library(cluster)
}
if(!require("dbscan")){
install.packages("dbscan")
library(dbscan)
}else{
library(dbscan)
}
if(!require("dplyr")){
install.packages("dplyr")
library(dplyr)
}else{
library(dplyr)
}
if(!require("factoextra")){
install.packages("factoextra")
library(factoextra)
}else{
library(factoextra)
}
if(!require("rpart")){
install.packages("rpart")
library(rpart)
}else{
library(rpart)
}
if(!require("rpart.plot")){
install.packages("rpart.plot")
library(rpart.plot)
}else{
library(rpart.plot)
}
if(!require("caret")){
install.packages("caret")
library(caret)
}else{
library(caret)
}
# Nos quedamos con las 6 componentes principales
data <- pca$x[,1:6]
# Definimos el rango de las variables
centros <- 1:6
# Inicializamos un vector para almacenar suma de las distancias cuadradas
wss <- numeric(length(centros))
# Iteramos sobre el rango para calcular wss por medoid
for (k in centros) {
set.seed(123)
clara_euc <- clara(data, k, metric="euclidean", samples=100)
wss[k] <- sum(clara_euc$objective)
}
# Creamos df relacionando numero de centros a su wss
codo <- data.frame(k = centros, wss = wss)
# Graficamos para cada valor del numero de medoids
ggplot(codo, aes(x = k, y = wss)) +
geom_line(color = "blue") +
geom_point(color = "red") +
ggtitle("Método del Codo") +
xlab("Número de Clusters (k)") +
ylab("Suma  Distancias Cuadradas Dentro de los Clusters")
# Establecemos la semilla de aleatoriedad
set.seed(123)
# Construimos el modelo clara, 4 centroides, 100 subconjuntos
clara_euc <- clara(data, 4, metric="euclidean", samples = 100)
# Calculamos los coeficientes de silueta de los puntos en sus clusters
silueta_scores <- silhouette(clara_euc$cluster, dist(data))
# Hacemos una media de los coeficientes de silueta
media_scores <- mean(silueta_scores[, 3])
# Mostramos la media resultado
media_scores
# Creamos un dataframe con los clusters y sus coeficientes de silueta
silueta_df <- data.frame(cluster = silueta_scores[, 1], sil_width = silueta_scores[, 3])
# Graficamos el resultado obtenido en un diagrama de cajas
ggplot(silueta_df, aes(x = factor(cluster), y = sil_width)) +
geom_boxplot() +
labs(title = "Coeficientes de Silueta por Cluster",
x = "Cluster",
y = "Ancho de Silueta") +
theme_minimal()
# Definimos el rango de las variables
centros <- 1:6
# Inicializamos un vector para almacenar suma de las distancias cuadradas
wss <- numeric(length(centros))
# Iteramos sobre el rango para calcular wss por centro
for (k in centros) {
set.seed(123)
clara_man <- clara(data, k, metric="manhattan", samples=100)
wss[k] <- sum(clara_man$objective)
}
# Creamos df relacionando numero de centros a su wss
codo <- data.frame(k = centros, wss = wss)
# Graficamos para cada valor de los medoids
ggplot(codo, aes(x = k, y = wss)) +
geom_line(color = "blue") +
geom_point(color = "red") +
ggtitle("Método del Codo") +
xlab("Número de Clusters (k)") +
ylab("Suma Distancias Cuadradas Dentro de los Clusters")
# Establecemos la semilla de aleatoriedad
set.seed(123)
# Construimos el modelo kmeans, 4 centroides, 25 iteraciones aleatorias
clara_man <- clara(data, 4, metric="manhattan", samples = 100)
# Calculamos los coeficientes de silueta de los puntos en sus clusters
silueta_scores <- silhouette(clara_man$cluster, dist(data))
# Hacemos una media de los coeficientes de silueta
media_scores <- mean(silueta_scores[, 3])
# Mostramos la media resultado
media_scores
# Creamos un dataframe con los clusters y sus coeficientes de silueta
silueta_df <- data.frame(cluster = silueta_scores[, 1], sil_width = silueta_scores[, 3])
# Graficamos el resultado obtenido en un diagrama de cajas
ggplot(silueta_df, aes(x = factor(cluster), y = sil_width)) +
geom_boxplot() +
labs(title = "Coeficientes de Silueta por Cluster",
x = "Cluster",
y = "Ancho de Silueta") +
theme_minimal()
# Establecemos vector de posibles minimos de puntos
minPts <- seq(6, 10, by = 1)
# Iteramos sobre los posibles valores del minimo de puntos para cluster
for (i in minPts){
k <- i # Minimo de puntos para cluster
kNNdist_plot <- kNNdistplot(data, k = k) # Grafico de representacion de distancias
kNNdist_plot
}
# Creamos un dataframe de prueba para los clsuters de DBSCAN
data_db <- data
data_db <- as.data.frame(data_db)
# Creamos el modelo DBSCAN
dbscan_result <- dbscan(data_db, eps = 1, minPts = 7)
# Añadimos al dataframe los clusters de DBSCAN
data_db$cluster <- dbscan_result$cluster
# Calculamos los coeficientes de silueta de las variables con sus clusters
silhouette_scores <- silhouette(data_db$cluster, dist(data_db[, 1:6]))
# Visualizar el gráfico de silueta
fviz_silhouette(silhouette_scores)
# Generamos secuencias de valores eps y minPts
minPts <- seq(6, 10, by = 1)
eps <- seq(1.5, 2, by = 0.1)
# Creamos un dataframe para almacenar valores
resultados <- data.frame(eps = numeric(), minPts = numeric(), avg_silhouette = numeric(), num_clusters = numeric(), stringsAsFactors = FALSE)
# Iteramos sobre los valores para comprobar los resultados de dbscan
for (i in minPts){
for (j in eps){
data_db <- as.data.frame(data)
# Creamos el DBSCAN
dbscan_result <- dbscan(data_db, eps = j, minPts = i)
# Añadimos los clusters
data_db$cluster <- dbscan_result$cluster
# Calculamos numero de clusters
clusters <- length(unique(dbscan_result$cluster))-1
# Añadimos condicion para establecer minimo de clusters
if (clusters >= 9 || clusters < 2){
next
} else{
# Mostramos si se ha añade un nuevo valor al dataframe
cat("Se ha añadido un nuevo valor eps =", j, "y para minPts =", i, "\n")
# Calculamos las siluetas
silhouette_scores <- silhouette(data_db$cluster, dist(data_db[, 1:6]))
avg_silhouette <- mean(silhouette_scores[, 3])
# Cargamos en dataframe
resultados <- rbind(resultados, data.frame(eps = j, minPts = i, avg_silhouette = avg_silhouette, num_clusters = clusters))
}
}
}
#Imprimimos el final de la iteracion
print("Terminada la iteracion!")
resultados[resultados$avg_silhouette == max(resultados$avg_silhouette), ]
# Creamos los dataframes de pruebas
data_db <- as.data.frame(data)
# Creamos el DBSCAN
dbscan_result <- dbscan(data_db, eps = 2, minPts = 7)
# Añadimos los clusters al dataframe
data_db$cluster <- dbscan_result$cluster
# Calculamos las siluetas
silhouette_scores <- silhouette(data_db$cluster, dist(data_db[, 1:6]))
avg_silhouette <- mean(silhouette_scores[, 3])
# Mostramos las graficas de las siluetas
fviz_silhouette(silhouette_scores)
# Comprobamos el numero de registros de ruido respecto al total de registros
puntos_ruido <- (sum(data_db$cluster == 0))*100 / nrow(data_db)
puntos_ruido
# Creamos dataframe de prueba para optics
data_op <- as.data.frame(data)
#
knndist <- kNNdist(data_op, k = 7)
#
qplot(1:length(sort(knndist)), sort(knndist), geom = "line") +
labs(x = "Puntos ordenados", y = paste("Distancia al", 7, "-ésimo vecino más cercano"),
title = "Gráfica de distancias k-NN para determinar eps_cl")
minPts <- seq(6, 7, by = 1)
eps <- seq(0.5, 1, by = 0.1)
# Creamos un dataframe para almacenar valores
resultados_op <- data.frame(eps_cl = numeric(), minPts = numeric(), avg_silhouette = numeric(), num_clusters = numeric(), stringsAsFactors = FALSE)
# Iteramos sobre los valores para comprobar los resultados de dbscan
for (i in minPts){
for (j in eps){
data_op <- as.data.frame(data)
# Creamos el DBSCAN
optics_result <- optics(data_op, eps = j, minPts = i)
# Extraemos los clusters
clusters <- extractDBSCAN(optics_result, eps_cl = j)
# Añadimos los clusters
data_op$cluster <- clusters$cluster
# Calculamos numero de clusters
clusters <- length(unique(clusters$cluster))-1
# Añadimos condicion para establecer minimo de clusters
if (clusters >= 9 || clusters < 2){
next
} else{
# Mostramos si se ha añade un nuevo valor al dataframe
cat("Se ha añadido un nuevo valor eps =", j, "y para minPts =", i, "\n")
# Calculamos las siluetas
silhouette_scores <- silhouette(data_op$cluster, dist(data_op[, 1:6]))
avg_silhouette <- mean(silhouette_scores[, 3])
# Cargamos en dataframe
resultados <- rbind(resultados, data.frame(eps = j, minPts = i, avg_silhouette = avg_silhouette, num_clusters = clusters))
}
}
}
#Imprimimos el final de la iteracion
print("Terminada la iteracion!")
minPts <- seq(6, 7, by = 1)
eps <- seq(0.5, 1.5, by = 0.1)
# Creamos un dataframe para almacenar valores
resultados_op <- data.frame(eps_cl = numeric(), minPts = numeric(), avg_silhouette = numeric(), num_clusters = numeric(), stringsAsFactors = FALSE)
# Iteramos sobre los valores para comprobar los resultados de dbscan
for (i in minPts){
for (j in eps){
data_op <- as.data.frame(data)
# Creamos el DBSCAN
optics_result <- optics(data_op, eps = j, minPts = i)
# Extraemos los clusters
clusters <- extractDBSCAN(optics_result, eps_cl = j)
# Añadimos los clusters
data_op$cluster <- clusters$cluster
# Calculamos numero de clusters
clusters <- length(unique(clusters$cluster))-1
# Añadimos condicion para establecer minimo de clusters
if (clusters >= 9 || clusters < 2){
next
} else{
# Mostramos si se ha añade un nuevo valor al dataframe
cat("Se ha añadido un nuevo valor eps =", j, "y para minPts =", i, "\n")
# Calculamos las siluetas
silhouette_scores <- silhouette(data_op$cluster, dist(data_op[, 1:6]))
avg_silhouette <- mean(silhouette_scores[, 3])
# Cargamos en dataframe
resultados <- rbind(resultados, data.frame(eps = j, minPts = i, avg_silhouette = avg_silhouette, num_clusters = clusters))
}
}
}
#Imprimimos el final de la iteracion
print("Terminada la iteracion!")
minPts <- seq(6, 10, by = 1)
eps <- seq(1, 2, by = 0.1)
# Creamos un dataframe para almacenar valores
resultados_op <- data.frame(eps_cl = numeric(), minPts = numeric(), avg_silhouette = numeric(), num_clusters = numeric(), stringsAsFactors = FALSE)
# Iteramos sobre los valores para comprobar los resultados de dbscan
for (i in minPts){
for (j in eps){
data_op <- as.data.frame(data)
# Creamos el DBSCAN
optics_result <- optics(data_op, eps = j, minPts = i)
# Extraemos los clusters
clusters <- extractDBSCAN(optics_result, eps_cl = j)
# Añadimos los clusters
data_op$cluster <- clusters$cluster
# Calculamos numero de clusters
clusters <- length(unique(clusters$cluster))-1
# Añadimos condicion para establecer minimo de clusters
if (clusters >= 9 || clusters < 2){
next
} else{
# Mostramos si se ha añade un nuevo valor al dataframe
cat("Se ha añadido un nuevo valor eps =", j, "y para minPts =", i, "\n")
# Calculamos las siluetas
silhouette_scores <- silhouette(data_op$cluster, dist(data_op[, 1:6]))
avg_silhouette <- mean(silhouette_scores[, 3])
# Cargamos en dataframe
resultados <- rbind(resultados, data.frame(eps = j, minPts = i, avg_silhouette = avg_silhouette, num_clusters = clusters))
}
}
}
#Imprimimos el final de la iteracion
print("Terminada la iteracion!")
resultados[resultados$avg_silhouette == max(resultados$avg_silhouette), ]
# Comprobamos el numero de registros de ruido respecto al total de registros
puntos_ruido <- (sum(data_db$cluster == 0))*100 / nrow(data_db)
puntos_ruido
# Creamos los dataframes de pruebas
data_op <- as.data.frame(data)
# Creamos el DBSCAN
optics_result <- optics(data_op, eps = 2, minPts = 7)
# Extraemos los clusters mediante la fucnion extractDBSCAN
clusters <- extractDBSCAN(optics_result, eps_cl = 2)
# Añadimos los clusters al dataframe
data_op$cluster <- clusters$cluster
º# Calculamos las siluetas
# Creamos los dataframes de pruebas
data_op <- as.data.frame(data)
# Creamos el DBSCAN
optics_result <- optics(data_op, eps = 2, minPts = 7)
# Extraemos los clusters mediante la fucnion extractDBSCAN
clusters <- extractDBSCAN(optics_result, eps_cl = 2)
# Añadimos los clusters al dataframe
data_op$cluster <- clusters$cluster
# Calculamos las siluetas
silhouette_scores <- silhouette(data_op$cluster, dist(data_op[, 1:6]))
avg_silhouette <- mean(silhouette_scores[, 3])
# Mostramos las graficas de las siluetas
fviz_silhouette(silhouette_scores)
# Creamos los dataframes de pruebas
data_op <- as.data.frame(data)
# Creamos el modelo OPTICS
optics_result <- optics(data_op, eps = 2, minPts = 7)
# Extraemos los clusters mediante la funcion extractDBSCAN
clusters <- extractDBSCAN(optics_result, eps_cl = 2)
# Añadimos los clusters al dataframe
data_op$cluster <- clusters$cluster
# Calculamos las siluetas
silhouette_scores <- silhouette(data_op$cluster, dist(data_op[, 1:6]))
avg_silhouette <- mean(silhouette_scores[, 3])
# Mostramos las graficas de las siluetas
fviz_silhouette(silhouette_scores)
# Comprobamos el numero de registros de ruido respecto al total de registros
puntos_ruido <- (sum(data_op$cluster == 0))*100 / nrow(data_db)
puntos_ruido
knitr::opts_chunk$set(echo = TRUE)
if (!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')
if (!require('corrplot')) install.packages('corrplot'); library('corrplot')
# Establecemos el directorio
setwd("C:\Users\alvar\Desktop\SALUD_MENTAL")
# Establecemos el directorio
setwd("C:/Users/alvar/Desktop/SALUD_MENTAL")
# Leemos el dataset
df <- read.csv("./data/master.csv")
# Imprimimos estructura del mismo
str(df)
# Establecemos el directorio
setwd("./Desktop/SALUD_MENTAL")
# Establecemos el directorio
setwd("C:/Users/alvar/Desktop/SALUD_MENTAL")
# Leemos el dataset
df <- read.csv("./data/master.csv")
# Imprimimos estructura del mismo
str(df)
